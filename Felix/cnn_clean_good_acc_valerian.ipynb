{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "97ebd0e6-e0a7-432e-bd2f-476fd98b53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import shutil\n",
    "import stat\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5d77be3b-fce6-479d-be15-1d50b908a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available on this machine\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Name of the Graphics card\", torch.cuda.get_device_name())\n",
    "    print(\"Number of GPU available\", torch.cuda.device_count())\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"No GPU available on this machine\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf20e11f-9ba6-45cf-bccd-2f2a5d12cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def accuracy(predicted_logits, reference):\n",
    "    \"\"\"\n",
    "    Compute the ratio of correctly predicted labels\n",
    "    \n",
    "    @param predicted_logits: float32 tensor of shape (batch size, num classes)\n",
    "    @param reference: int64 tensor of shape (batch_size) with the class number\n",
    "    \"\"\"\n",
    "    labels = torch.argmax(predicted_logits, 1)\n",
    "    correct_predictions = labels.eq(reference)\n",
    "    return correct_predictions.sum().float() / correct_predictions.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32689acb-9112-475d-90f8-1718cf877d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tests passed\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy():\n",
    "    #predictions = torch.tensor([[0.5, -0.6], [1.0, -0.3], [0.7, 0]]) #0\n",
    "    predictions = torch.tensor([[0.5, 1.0, 0.7], [-0.6, -0.3, 0]]) #1\n",
    "    correct_labels = torch.tensor([0, 2])  # first is wrong, second is correct\n",
    "    print()\n",
    "    assert accuracy(predictions, correct_labels).allclose(torch.tensor([0.5]))\n",
    "\n",
    "    #predictions = torch.tensor([[0.5, -0.6, -1], [1.0, -0.3, 0], [0.7, 0, 1]]) #0\n",
    "    predictions = torch.tensor([[0.5, 1.0, 0.7], [-0.6, -0.3, 0], [-1, 0, 1]]) #1\n",
    "    correct_labels = torch.tensor([1, 1, 2])  # correct, wrong, correct\n",
    "    assert accuracy(predictions, correct_labels).allclose(torch.tensor([2/3]))\n",
    "\n",
    "    print(\"Tests passed\")\n",
    "  \n",
    "test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7d2e7dc7-dd9c-49f8-abdc-0c751b182d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 images\n",
      "satImage_001.png\n",
      "Loading 20 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"data/training/\"\n",
    "image_test = \"data/test_set_images\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(20, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "n = 10 # Only use 10 images for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e73d",
   "metadata": {},
   "source": [
    "#### Training images patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65b618ba-33e4-4c5d-b2f2-38b2b80097ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from input training images\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "\n",
    "img_patches = [img_crop(imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "gt_patches = [img_crop(gt_imgs[i], patch_size, patch_size) for i in range(n)]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6f20135-f2e7-41cd-8ed4-dce1f0ea008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 6-dimensional features consisting of average RGB color as well as variance\n",
    "def extract_features(img):\n",
    "    feat_m = np.mean(img, axis=(0,1))\n",
    "    feat_v = np.var(img, axis=(0,1))\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract 2-dimensional features consisting of average gray color as well as variance\n",
    "def extract_features_2d(img):\n",
    "    feat_m = np.mean(img)\n",
    "    feat_v = np.var(img)\n",
    "    feat = np.append(feat_m, feat_v)\n",
    "    return feat\n",
    "\n",
    "# Extract features for a given image\n",
    "def extract_img_features(filename):\n",
    "    img = load_image(filename)\n",
    "    img_patches = img_crop(img, patch_size, patch_size)\n",
    "    X = np.asarray([ extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180084c0",
   "metadata": {},
   "source": [
    "#### Test files extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b782032f-5729-47a1-80a4-bb1171e4ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test files extracted from subfolders | Other unnecessary files removed\n",
      "Loading 20 images\n",
      "data\\test_set_images\\test_1.png\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'training/images')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'training/groundtruth')\n",
    "\n",
    "#x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "#y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "# extracting images from subfolders\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test_set_images/')\n",
    "os.chmod(x_test_dir , stat.S_IWRITE)\n",
    "folder = x_test_dir\n",
    "subfolders = [f.path for f in os.scandir(folder) if f.is_dir()]\n",
    "\n",
    "for sub in subfolders:\n",
    "    for f in os.listdir(sub):\n",
    "        if '.png' in f:\n",
    "            src = os.path.join(sub, f)\n",
    "            dst = os.path.join(folder, f)\n",
    "            shutil.move(src, dst)\n",
    "        else:\n",
    "            to_delete = os.path.join(sub,f)\n",
    "            os.remove(to_delete)\n",
    "\n",
    "# remove unnecessary files\n",
    "for file in os.listdir(x_test_dir):\n",
    "    if '.ini' in file:\n",
    "        file_to_remove_dir = os.path.join(x_test_dir, 'desktop.ini')\n",
    "        os.remove(file_to_remove_dir)\n",
    "print(\"Test files extracted from subfolders | Other unnecessary files removed\")\n",
    "\n",
    "test_files = os.listdir(x_test_dir)\n",
    "m = min(20, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(m) + \" images\")\n",
    "test_imgs = list(glob.iglob(x_test_dir + '*.png', recursive=True))\n",
    "print(test_imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fcc7e",
   "metadata": {},
   "source": [
    "#### Extracting patches from test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43b73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from input test images\n",
    "patch_size = 16 # each patch is 16*16 pixels\n",
    "\n",
    "test_img_patches = [img_crop(imgs[i], patch_size, patch_size) for i in range(m)]\n",
    "test_img_patches = np.asarray([test_img_patches[i][j] for i in range(len(test_img_patches)) for j in range(len(test_img_patches[i]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7828e9d",
   "metadata": {},
   "source": [
    "#### Train & Test classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc986ff6-4637-4cc2-bc17-868f956223ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsTrainset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            X, \n",
    "            y, \n",
    "    ):        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = self.X[i]\n",
    "        X = self.transform(image)\n",
    "        mask = self.y[i]\n",
    "        y = torch.from_numpy(np.asarray(mask)).long()\n",
    "        return X, y\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    #Transformation, used for data augmentation\n",
    "    transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ed5e598-eac2-4a0f-9323-c4127e6569bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsTestset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            X, \n",
    "    ):        \n",
    "        self.X = X\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks        \n",
    "        image = self.X[i]\n",
    "        X = self.transform(image)\n",
    "        return X\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    #Transformation, used for data augmentation\n",
    "    transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a449962b-a4dc-4c56-9603-61507c62a49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6250, 2), (6250, 16, 16))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute features for each image patch\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def value_to_class(v):\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "X = np.asarray([ extract_features_2d(img_patches[i]) for i in range(len(img_patches))])\n",
    "Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))])\n",
    "\n",
    "#Onehot encode\n",
    "Y_onehot = np.array([[(1-i), i] for i in Y])\n",
    "Y_onehot.shape, gt_patches.shape\n",
    "\n",
    "# Balancing the data\n",
    "roads = img_patches[Y == 1]\n",
    "nb_roads = len(roads)\n",
    "backgrounds = img_patches[Y == 0]\n",
    "nb_backgrounds = len(backgrounds)\n",
    "\n",
    "training = []\n",
    "labels = []\n",
    "for i in range (min(nb_roads, nb_backgrounds)):\n",
    "    training.append(roads[i % nb_roads])\n",
    "    training.append(backgrounds[i % nb_backgrounds])\n",
    "    labels.append(1)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a070100-f370-41bf-b9ec-2e84ac475310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "batch_size = 10\n",
    "dataset_train = torch.utils.data.DataLoader(RoadsTrainset(img_patches, Y), batch_size=batch_size, shuffle=True)\n",
    "dataset_test = torch.utils.data.DataLoader(RoadsTestset(img_patches), batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65c6c7fa-bba8-44b9-a632-052bc47cb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"From: LeCun et al., 1998. Gradient-Based Learning Applied to Document Recognition\"\"\"\n",
    "    super().__init__()\n",
    "    self.conv1 = torch.nn.Conv2d(3, 200, kernel_size=5)\n",
    "    self.conv2 = torch.nn.Conv2d(200, 400, kernel_size=5)\n",
    "    self.conv2_drop = torch.nn.Dropout2d(0.5)\n",
    "    self.fc1 = torch.nn.Linear(400, 100)\n",
    "    self.fc2 = torch.nn.Linear(100, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    relu = torch.nn.functional.relu\n",
    "    max_pool2d = torch.nn.functional.max_pool2d\n",
    "\n",
    "    x = relu(max_pool2d(self.conv1(x), 2))\n",
    "    x = relu(max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "    x = x.view(batch_size, -1)\n",
    "    x = relu(self.fc1(x))\n",
    "    x = torch.nn.functional.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    return x # Previously there was torch.nn.functional.log_softmax(x, dim=1) here which was incorrect (although the network could still train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55376595-7ad8-45c5-bdeb-0a2c79ef487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs):\n",
    "  \"\"\"\n",
    "  @param model: torch.nn.Module\n",
    "  @param criterion: torch.nn.modules.loss._Loss\n",
    "  @param dataset_train: torch.utils.data.DataLoader\n",
    "  @param dataset_test: torch.utils.data.DataLoader\n",
    "  @param optimizer: torch.optim.Optimizer\n",
    "  @param num_epochs: int\n",
    "  \"\"\"\n",
    "  print(\"Starting training\")\n",
    "  for epoch in range(num_epochs):\n",
    "    # Train an epoch\n",
    "    model.train()\n",
    "    averageLoss = 0\n",
    "    for batch_x, batch_y in dataset_train:\n",
    "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "      prediction = model(batch_x)\n",
    "      loss = criterion(prediction, batch_y)\n",
    "    \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      averageLoss += loss.item()/len(dataset_train)\n",
    "\n",
    "\n",
    "    # Test the quality on the test set\n",
    "    model.eval()\n",
    "    accuracies_test = []\n",
    "    i=0\n",
    "    # to calculate f1 score\n",
    "    target_true = 0\n",
    "    predicted_true = 0\n",
    "    correct_true = 0\n",
    "    for batch_x, batch_y in dataset_test:\n",
    "      with torch.no_grad():\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Evaluate the network (forward pass)\n",
    "        prediction = model(batch_x)\n",
    "\n",
    "        # calculating f1 score\n",
    "        predicted_classes = torch.argmax(prediction, dim=1) == 0\n",
    "        target_classes = batch_y\n",
    "        target_true += torch.sum(target_classes == 0).float()\n",
    "        predicted_true += torch.sum(predicted_classes).float()\n",
    "\n",
    "        correct_true_array = torch.logical_and(predicted_classes == target_classes, predicted_classes == 0)\n",
    "        correct_true_array = correct_true_array.type(torch.ByteTensor)\n",
    "        correct_true += torch.sum(correct_true_array).float()\n",
    "        if i==-5:\n",
    "              print(prediction, batch_y)\n",
    "        accuracies_test.append(accuracy(prediction, batch_y))\n",
    "        i+=1\n",
    "      \n",
    "  recall = correct_true / target_true\n",
    "  precision = correct_true / predicted_true\n",
    "  f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "  print(\"Test accuracy: {:.5f} | F1 Score: {:.3f}\".format(sum(accuracies_test).item()/len(accuracies_test), f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98d0b38e-9e71-4f0b-af2f-c50668c58252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Test accuracy: 0.90128 | F1 Score: 0.043\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model_lenet = LeNetModel().to(device)\n",
    "optimizer = torch.optim.Adam(model_lenet.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model_lenet, criterion, dataset_train, dataset_train, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e404b7-0e2d-44d3-8792-23e524bc0746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1fcff30e4e0f500db5efe77283d945f32ac3395cc1e1f4aa797d226dacc920e"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
